{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944ffe31",
   "metadata": {},
   "source": [
    "# Assignment 7: Clinical NLP with LLMs and Embeddings\n",
    "\n",
    "Extract structured data from clinical notes using LLM prompt engineering, then build a semantic search system using sentence embeddings.\n",
    "\n",
    "**Dataset:** 75 synthetic discharge summaries from [Asclepius-Synthetic-Clinical-Notes](https://huggingface.co/datasets/aisc-team-a1/Asclepius-Synthetic-Clinical-Notes) (Kweon et al., 2023) in `asclepius_notes.json`.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feac8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt\n",
    "\n",
    "# Clear state after installing packages. If you re-run cells out of order later, re-run this cell first.\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73f8459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "load_dotenv()\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b2f75",
   "metadata": {},
   "source": [
    "### API Key\n",
    "\n",
    "Part 1 requires an [OpenRouter](https://openrouter.ai) API key (OpenAI keys also work). Add the key from class forum to `.env` (not `example.env`). It should look like:\n",
    "\n",
    "```bash\n",
    "OPENROUTER_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "### Helper Functions (modify at your own risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf760aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM client setup (do not modify) ---\n",
    "\n",
    "def get_client():\n",
    "    \"\"\"Initialize the LLM client based on available API keys.\"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    if os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "        client = OpenAI(\n",
    "            api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "        )\n",
    "        return client, \"openrouter\"\n",
    "\n",
    "    if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        return OpenAI(), \"openai\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"No API key found. Set OPENROUTER_API_KEY or OPENAI_API_KEY in .env\"\n",
    "    )\n",
    "\n",
    "\n",
    "def call_llm(prompt, provider, client):\n",
    "    \"\"\"Send a prompt to the LLM and return the response text.\"\"\"\n",
    "    model = \"openai/gpt-4o-mini\" if provider == \"openrouter\" else \"gpt-4o-mini\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical information extraction assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Detect the best available device for local model inference.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c0f89",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab44793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75 synthetic clinical notes\n",
      "Keys: ['patient_id', 'note']\n"
     ]
    }
   ],
   "source": [
    "with open(\"asclepius_notes.json\") as f:\n",
    "    asclepius = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(asclepius)} synthetic clinical notes\")\n",
    "print(f\"Keys: {list(asclepius[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc4529e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discharge Summary\n",
      "\n",
      "Patient Name: N/A\n",
      "Date of Admission: N/A\n",
      "Date of Discharge: N/A\n",
      "\n",
      "Hospital Course:\n",
      "\n",
      "This patient was admitted with left back pain which had been persistent for over 6 months and had recently aggravated. Radiological examination revealed a cavitary lesion in the left lower lobe of the lung that was indicative of pulmonary chronic inflammation. The lesion was confirmed to be hamartoma after histology revealed the diagnosis. \n",
      "Subsequently, a left lower lobectomy was performed. The...\n"
     ]
    }
   ],
   "source": [
    "print(asclepius[0][\"note\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5604dc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Clinical Entity Extraction\n",
    "\n",
    "Use LLM prompt engineering to extract structured medical data from clinical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e923cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4 notes for extraction\n",
      "\n",
      "--- Note 1 (1624 chars) ---\n",
      "Hospital Course:\n",
      "\n",
      "The patient was a 27-year-old pregnant woman who presented with recurrent panic attacks and other anxiety symptoms. On evaluation, i...\n",
      "\n",
      "--- Note 2 (1119 chars) ---\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [Redacted]\n",
      "Age: 35\n",
      "Sex: Male\n",
      "\n",
      "Clinical Course:\n",
      "\n",
      "The patient was admitted to our hospital after reporting an episode ...\n",
      "\n",
      "--- Note 3 (1459 chars) ---\n",
      "Hospital Course Summary:\n",
      "\n",
      "Patient was admitted for multiple warty lesions with severe pruritus on the lower legs and dorsa of feet. The lesions had be...\n",
      "\n",
      "--- Note 4 (1250 chars) ---\n",
      "Hospital Course:\n",
      "The patient, a 23-year-old male with no known comorbidities, presented with complaints of epigastric pain and frequent bilious vomiti...\n"
     ]
    }
   ],
   "source": [
    "# Select 4 notes for extraction\n",
    "random.seed(2026)\n",
    "sample = random.sample(asclepius, 4)\n",
    "notes_p1 = [s[\"note\"] for s in sample]\n",
    "\n",
    "print(f\"Selected {len(notes_p1)} notes for extraction\")\n",
    "for i, n in enumerate(notes_p1, 1):\n",
    "    print(f\"\\n--- Note {i} ({len(n)} chars) ---\")\n",
    "    print(n[:150] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5907063",
   "metadata": {},
   "source": [
    "### `build_prompt`\n",
    "\n",
    "Build a prompt that instructs the LLM to extract structured data from a clinical note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602d2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement build_prompt\n",
    "def build_prompt(note, few_shot=False):\n",
    "    \n",
    "    schema = json.dumps({\n",
    "        \"diagnosis\": \"<Primary diagnosis as a string>\",\n",
    "        \"medications\": [\"<List of medications mentioned in the note>\"],\n",
    "        \"lab_values\": {\"<Lab test name>\": \"<Value and units>\"},\n",
    "        \"confidence\": \"<Float value between 0.0 and 1.0>\"\n",
    "    }, indent = 2)\n",
    "\n",
    "    example = \"\"\n",
    "\n",
    "    if few_shot:\n",
    "        example = \"\"\"\n",
    "        \n",
    "Example Note 1:\n",
    "\n",
    "    Note: The patient was a 27-year-old pregnant woman who presented with recurrent panic attacks and other anxiety symptoms. \n",
    "    Labs showed Hemoglobin A1c of 5.6% and elevated cortisol levels. \n",
    "    She was diagnosed with Generalized Anxiety Disorder and prescribed Sertraline 50mg daily.\n",
    "\n",
    "    Output:{\n",
    "    \"diagnosis\": \"Generalized Anxiety Disorder\",\n",
    "    \"medications\": [\"Sertraline 50mg daily\"],\n",
    "    \"lab_values\": {\n",
    "        \"Hemoglobin A1c\": \"5.6%\",\n",
    "        \"Cortisol\": \"elevated\"\n",
    "    },\n",
    "    \"confidence\": 0.95\n",
    "    }\n",
    "        \n",
    "Example Note 2:\n",
    "\n",
    "    Note: A 34-year-old woman presented with a two-week history of fatigue, joint pain, and a butterfly-shaped facial rash. \n",
    "    ANA was positive at 1:640, anti-dsDNA was elevated. \n",
    "    She was started on hydroxychloroquine and low-dose prednisone.\n",
    "\n",
    "    Output:{\n",
    "    \"diagnosis\": \"Systemic Lupus Erythematosus\",\n",
    "    \"medications\": [\"Hydroxychloroquine\", \"Low-dose prednisone\"],\n",
    "    \"lab_values\": {\n",
    "        \"ANA\": \"positive at 1:640\",\n",
    "        \"Anti-dsDNA\": \"elevated\"\n",
    "    },\n",
    "    \"confidence\": 0.92\n",
    "    }\"\"\"\n",
    "        \n",
    "    return f\"\"\"Extract structured medical information from the following clinical note.\n",
    "Return ONLY a JSON object matching this schema:\n",
    "{schema}\n",
    "\n",
    "Guidelines:\n",
    "- If \"medications\" are REDACTED or unnamed, return \"Medication REDACTED\" or \"Medication Unnamed\" accordingly\n",
    "- For \"lab_values\" allow both qualitative and quantitative values for test results (e.g. \"5.6%\", \"elevated\", \"normal\")\n",
    "- \"confidence\" between 0.0 and 1.0, with 0.0 being no confidence and 1.0 being complete confidence in the accuracy of the extracted information. \n",
    "- Set \"confidence\" higher when any field value is explicitly stated, and lower when they are implied or uncertain. \n",
    "- Penalize \"confidence\" for note length, as longer notes contain more complex information and more room for error.\n",
    "- If any field cannot be extracted, return \"None\" for that field.\n",
    "\n",
    "{example}\n",
    "\n",
    "Extract from this note:\n",
    "{note}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67c11b",
   "metadata": {},
   "source": [
    "### `parse_json_response`\n",
    "\n",
    "Extract a JSON object from LLM response text, which may contain markdown code fences or other wrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e38847c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement parse_json_response\n",
    "def parse_json_response(text):\n",
    "\n",
    "    # Handle clean JSON strings\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Handle JSON wrapped in ```json ... ``` markdown blocks\n",
    "    if \"```\" in text:\n",
    "        lines = text.split(\"```\")\n",
    "        for blocks in lines[1::2]:\n",
    "            blocks = blocks.strip().removeprefix(\"json\").strip()\n",
    "            try:\n",
    "                return json.loads(blocks)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Find JSON within surrounding text (look for outermost { and })\n",
    "    start_curly = text.find(\"{\")\n",
    "    end_curly = text.rfind(\"}\")\n",
    "    if start_curly >=0 and end_curly > start_curly:\n",
    "        try:\n",
    "            return json.loads(text[start_curly:end_curly+1])\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # If all parsing attempts fail, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12163e8",
   "metadata": {},
   "source": [
    "### `validate_response`\n",
    "\n",
    "Check that a parsed response dict contains all required keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ffca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement validate_response\n",
    "def validate_response(response):\n",
    "    if type(response) is not dict:\n",
    "        return False\n",
    "    fields = {\"diagnosis\", \"medications\", \"lab_values\", \"confidence\"}\n",
    "    return all(field in response for field in fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1762c",
   "metadata": {},
   "source": [
    "### `extract_entities`\n",
    "\n",
    "Orchestrate the full extraction pipeline: get client, build prompt, call LLM, parse, validate, return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87156a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement extract_entities\n",
    "def extract_entities(note, few_shot=False):\n",
    "    client, provider = get_client()\n",
    "    prompt = build_prompt(note, few_shot = few_shot)\n",
    "    raw = call_llm(prompt, provider = provider, client = client)\n",
    "    parsed = parse_json_response(raw)\n",
    "    if validate_response(parsed):\n",
    "        return parsed\n",
    "    return None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0f62e",
   "metadata": {},
   "source": [
    "### Test extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488e998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Note 1 ---\n",
      "{\n",
      "  \"diagnosis\": \"Generalized Anxiety Disorder\",\n",
      "  \"medications\": [\n",
      "    \"Medication Unnamed\"\n",
      "  ],\n",
      "  \"lab_values\": {\n",
      "    \"Thyroid Profile\": \"normal\",\n",
      "    \"Ultrasound\": \"normal\",\n",
      "    \"Electrocardiogram\": \"normal\",\n",
      "    \"Blood Pressure\": \"normal\"\n",
      "  },\n",
      "  \"confidence\": 0.85\n",
      "}\n",
      "\n",
      "--- Note 2 ---\n",
      "{\n",
      "  \"diagnosis\": \"Anaphylaxis\",\n",
      "  \"medications\": [\n",
      "    \"Medication Unnamed\"\n",
      "  ],\n",
      "  \"lab_values\": {\n",
      "    \"None\": \"None\"\n",
      "  },\n",
      "  \"confidence\": 0.75\n",
      "}\n",
      "\n",
      "--- Note 3 ---\n",
      "{\n",
      "  \"diagnosis\": \"Hypertrophic Lichen Planus\",\n",
      "  \"medications\": [\n",
      "    \"Medication REDACTED\"\n",
      "  ],\n",
      "  \"lab_values\": {\n",
      "    \"Routine Hematological and Biochemical Investigations\": \"normal\"\n",
      "  },\n",
      "  \"confidence\": 0.85\n",
      "}\n",
      "\n",
      "--- Note 4 ---\n",
      "{\n",
      "  \"diagnosis\": \"Type II choledochal cyst/double GB/Type VI choledochal cyst\",\n",
      "  \"medications\": [\n",
      "    \"Medication Unnamed\"\n",
      "  ],\n",
      "  \"lab_values\": {\n",
      "    \"Imaging studies\": \"hypoechoic lesion located around the porta hepatis\"\n",
      "  },\n",
      "  \"confidence\": 0.85\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_p1 = []\n",
    "for i, note in enumerate(notes_p1, 1):\n",
    "    result = extract_entities(note, few_shot=True)\n",
    "    print(f\"--- Note {i} ---\")\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        results_p1.append(result)\n",
    "    else:\n",
    "        print(\"Extraction failed\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5957f3",
   "metadata": {},
   "source": [
    "### Save Part 1 results (do not modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10e4b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 extraction results to output/extraction_results.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"output/extraction_results.json\", \"w\") as f:\n",
    "    json.dump(results_p1, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(results_p1)} extraction results to output/extraction_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717dd34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Semantic Search\n",
    "\n",
    "Build a semantic search system that finds clinical notes by meaning rather than keywords, using sentence embeddings and cosine similarity.\n",
    "\n",
    "This part runs locally — no API key needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5b31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasblakely/07-transformers-roll-out-thomasblakely/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1388.40it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on mps\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=get_device())\n",
    "print(f\"Model loaded on {get_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa91280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 notes in search corpus\n"
     ]
    }
   ],
   "source": [
    "# Use all 75 notes for the search corpus\n",
    "notes_p2 = [n[\"note\"] for n in asclepius]\n",
    "print(f\"{len(notes_p2)} notes in search corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda13c04",
   "metadata": {},
   "source": [
    "### `embed_notes`\n",
    "\n",
    "Generate embeddings for a list of notes using the sentence transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2804b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement embed_notes\n",
    "def embed_notes(notes):\n",
    "    return model.encode(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa3f61",
   "metadata": {},
   "source": [
    "### `find_similar`\n",
    "\n",
    "Search notes by meaning using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bab447e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement find_similar\n",
    "def find_similar(query, notes, embeddings, top_k=2):\n",
    "    query_embedding = model.encode([query])\n",
    "    cosine_score = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    score_descending = np.argsort(cosine_score)[::-1]\n",
    "    return [{\"note\": notes[i], \"score\": float(cosine_score[i])} for i in score_descending[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dee702",
   "metadata": {},
   "source": [
    "### Run the search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0df63a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: (75, 384)\n",
      "\n",
      "Query: 'heart attack symptoms'\n",
      "  1. (score: 0.386) Discharge Summary\n",
      "\n",
      "Patient: 49-year-old Hispanic woman\n",
      "Admission Date: March 202...\n",
      "  2. (score: 0.367) Discharge Summary:\n",
      "\n",
      "Patient: 88-year-old female\n",
      "\n",
      "Admission: Coronary Care Unit\n",
      "\n",
      "...\n",
      "\n",
      "Query: 'infectious disease with fever'\n",
      "  1. (score: 0.412) Discharge Summary\n",
      "\n",
      "Patient: 44-year-old male with end-stage renal disease caused...\n",
      "  2. (score: 0.359) Hospital Course: \n",
      "\n",
      "The patient, a 9-month-old baby girl from Cameroon, was admit...\n",
      "\n",
      "Query: 'respiratory illness'\n",
      "  1. (score: 0.508) Discharge Summary:\n",
      "\n",
      "Patient Name: Not Provided\n",
      "Age: 71\n",
      "Sex: Female\n",
      "\n",
      "Admission Da...\n",
      "  2. (score: 0.490) Discharge Summary\n",
      "\n",
      "Patient: 52-year-old male internist with a positive SARS-CoV-...\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_notes(notes_p2)\n",
    "print(f\"Embeddings: {embeddings.shape}\")\n",
    "\n",
    "queries = [\n",
    "    \"heart attack symptoms\",\n",
    "    \"infectious disease with fever\",\n",
    "    \"respiratory illness\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: '{q}'\")\n",
    "    results = find_similar(q, notes_p2, embeddings, top_k=2)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. (score: {r['score']:.3f}) {r['note'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c261a4",
   "metadata": {},
   "source": [
    "### Save Part 2 results (do not modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a83a93f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3 search results to output/search_results.json\n"
     ]
    }
   ],
   "source": [
    "search_results = find_similar(\"heart attack symptoms\", notes_p2, embeddings, top_k=3)\n",
    "with open(\"output/search_results.json\", \"w\") as f:\n",
    "    json.dump(search_results, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(search_results)} search results to output/search_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c169b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa2789bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 'python -m pytest .github/tests/ -v' in your terminal to check your work.\n"
     ]
    }
   ],
   "source": [
    "print(\"Run 'python -m pytest .github/tests/ -v' in your terminal to check your work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9591c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Build a Tiny LLM *(optional, not graded)*\n",
    "\n",
    "Train a character-level transformer to generate new text from a dataset of short strings. This mirrors the microGPT demo from lecture — same architecture, different data, using PyTorch's built-in modules instead of writing everything from scratch.\n",
    "\n",
    "**Choose your dataset** (or use both!):\n",
    "\n",
    "| Dataset | File | Items | Description |\n",
    "|:---|:---|:---|:---|\n",
    "| D&D Spells | `dnd_spells.lst` | 518 | Official spell names from Dungeons & Dragons |\n",
    "| Ice Cream | `icecream_flavors.lst` | 450 | Ice cream flavor names from a [CMU student survey](https://www.cs.cmu.edu/~15110-f23/slides/all-icecream.csv) |\n",
    "\n",
    "The code below uses D&D spells — swap the filename and variable names if you prefer ice cream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e760abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06dc1f",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be1605f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519 items from dnd_spells.lst\n",
      "55 unique characters, 7348 total tokens\n",
      "Vocabulary: \n",
      " '-/ABCDEFGHIJKLMNOPQRSTUVWZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Choose your dataset: \"dnd_spells.lst\" or \"icecream_flavors.lst\"\n",
    "datafile = \"dnd_spells.lst\"\n",
    "\n",
    "with open(datafile) as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "items = [line.strip() for line in lines[1:] if line.strip()]  # skip header\n",
    "\n",
    "text = \"\\n\".join(items)\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character <-> integer mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"{len(items)} items from {datafile}\")\n",
    "print(f\"{len(chars)} unique characters, {len(data)} total tokens\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c799737",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "This is a minimal GPT: token embeddings + position embeddings → transformer decoder → output head. Read through the code, then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aee0a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharGPT: 142,775 parameters on mps\n"
     ]
    }
   ],
   "source": [
    "block_size = 32   # context window (characters)\n",
    "n_embd = 64       # embedding dimension\n",
    "n_head = 4        # attention heads\n",
    "n_layer = 2       # transformer blocks\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "class CharGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each character gets a learnable vector of size n_embd\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        # Each position (0..block_size-1) also gets a learnable vector\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack of transformer decoder layers — this is where attention happens\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=4 * n_embd,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=n_layer)\n",
    "\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        # Project from embedding space back to vocabulary size (one logit per character)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok = self.tok_emb(idx)                                    # (B, T, n_embd)\n",
    "        pos = self.pos_emb(torch.arange(T, device=idx.device))    # (T, n_embd)\n",
    "        x = self.drop(tok + pos)                                   # (B, T, n_embd)\n",
    "\n",
    "        # Causal mask: prevents each position from attending to future positions\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T, device=idx.device)\n",
    "        x = self.transformer(x, x, tgt_mask=mask, memory_mask=mask)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "char_model = CharGPT().to(device)\n",
    "print(f\"CharGPT: {sum(p.numel() for p in char_model.parameters()):,} parameters on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ca424",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "The training loop samples random chunks from the data and teaches the model to predict the next character. Loss should drop below ~2.0 after 2000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e5425b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0 | loss 4.2356\n",
      "step  500 | loss 2.4813\n",
      "step 1000 | loss 2.2988\n",
      "step 1500 | loss 2.0728\n",
      "step 1999 | loss 1.9269\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(char_model.parameters(), lr=3e-4)\n",
    "batch_size = 32\n",
    "steps = 2000\n",
    "\n",
    "for step in range(steps):\n",
    "    # Pick random starting positions\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix]).to(device)\n",
    "\n",
    "    logits, loss = char_model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0 or step == steps - 1:\n",
    "        print(f\"step {step:4d} | loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c249d6",
   "metadata": {},
   "source": [
    "### Generate\n",
    "\n",
    "Sample from the trained model at different temperatures. Lower temperature = more conservative (common patterns), higher = more creative (weirder output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9419e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature 0.5 ---\n",
      "  Prison Spher\n",
      "  Prind Bof Ward\n",
      "  Wind Walls\n",
      "  Prdatic Wof Shad\n",
      "  Prmon Sphikon\n",
      "  Pontinen Sprctore\n",
      "  Plthtinon Smade\n",
      "  Relentone Ward\n",
      "  Stoncere Sthad\n",
      "  Spimal Sphike\n",
      "\n",
      "--- Temperature 0.8 ---\n",
      "  Neate Bof Scity\n",
      "  Seonecting\n",
      "  Brane Spoum\n",
      "  Blll Gueade\n",
      "  Berouint Flam\n",
      "  Guisp\n",
      "  Grate Stond\n",
      "  Troummhe Wall\n",
      "  Show Mebstr\n",
      "  Sponake Poof Wasth\n",
      "\n",
      "--- Temperature 1.2 ---\n",
      "  Rater\n",
      "  Selemal\n",
      "  Fordiath Woand\n",
      "  Ammm N\n",
      "  Hof Belllad\n",
      "  Maghtuch OwecelnssPeant\n",
      "  Coriol Ppred Mitonss Mars\n",
      "  Pronjurcert Sthy\n",
      "  Pumomond Aeal\n",
      "  Wak's Spre\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, max_new_tokens=500, temperature=0.8):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[stoi[\"\\n\"]]], device=device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -block_size:]\n",
    "        logits, _ = model(context)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    model.train()\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "for temp in [0.5, 0.8, 1.2]:\n",
    "    print(f\"\\n--- Temperature {temp} ---\")\n",
    "    output = generate(char_model, temperature=temp)\n",
    "    names = [s.strip() for s in output.split(\"\\n\") if s.strip()]\n",
    "    for name in names[:10]:\n",
    "        print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001e7e5",
   "metadata": {},
   "source": [
    "### Experiment (optional)\n",
    "\n",
    "Try changing things and see what happens:\n",
    "\n",
    "- Switch datasets — do ice cream flavors vs spell names produce different quality output?\n",
    "- Increase `n_layer` to 4 or `n_embd` to 128 — does the model improve? How much slower is training?\n",
    "- Train for 5000 steps instead of 2000\n",
    "- What happens at very low temperature (0.2) vs very high (2.0)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
